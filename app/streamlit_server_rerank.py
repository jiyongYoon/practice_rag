import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from typing import List
from langchain_core.documents import Document

from model import ollama_llm_model, openai_embeddings_model
from splitter import text_splitter
from app.rag_chat import retriever
from retriever import rerank_retriever

# â­ï¸ Embedding ì„¤ì •
# USE_BGE_EMBEDDING = True ë¡œ ì„¤ì •ì‹œ HuggingFace BAAI/bge-m3 ì„ë² ë”© ì‚¬ìš© (2.7GB ë‹¤ìš´ë¡œë“œ ì‹œê°„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
# # USE_BGE_EMBEDDING = False ë¡œ ì„¤ì •ì‹œ OpenAIEmbeddings ì‚¬ìš© (OPENAI_API_KEY ì…ë ¥ í•„ìš”. ê³¼ê¸ˆ)
# USE_BGE_EMBEDDING = False
#
# if not USE_BGE_EMBEDDING:
#     # OPENAI API KEY ì…ë ¥
#     # Embedding ì„ ë¬´ë£Œ í•œê¸€ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ í•„ìš” ì—†ìŒ!
#     os.environ["OPENAI_API_KEY"] = "OPENAI API KEY ì…ë ¥"

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
# 1) REMOTE ì ‘ì†: ë³¸ì¸ì˜ REMOTE LANGSERVE ì£¼ì†Œ ì…ë ¥
# (ì˜ˆì‹œ)
# LANGSERVE_ENDPOINT = "https://poodle-deep-marmot.ngrok-free.app/llm/"
LANGSERVE_ENDPOINT = "https://cd4c-220-75-173-230.ngrok-free.app/llm/"

# 2) LocalHost ì ‘ì†: ëì— ë¶™ëŠ” N4XyA ëŠ” ê°ì ë‹¤ë¥´ë‹ˆ
# http://localhost:8000/llm/playground ì—ì„œ python SDK ì—ì„œ í™•ì¸!
# LANGSERVE_ENDPOINT = "http://localhost:8000/llm/c/N4XyA"

# # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
# if not os.path.exists(".cache"):
#     os.mkdir(".cache")
# if not os.path.exists(".cache/embeddings"):
#     os.mkdir(".cache/embeddings")
# if not os.path.exists(".cache/files"):
#     os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
RAG_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì„ í• ë•ŒëŠ” ê·¼ê±°ì˜ ì¶œì²˜ë¥¼ ë‹µë³€ ë’¤ì— ë¶™ì—¬ì£¼ì„¸ìš”. ëª¨ë“  ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ í•¨ê»˜ ë‹µë³€í•´ì•¼í•©ë‹ˆë‹¤.
ê·¼ê±°ì˜ ì¶œì²˜ëŠ” contextì— í•¨ê»˜ ì „ë‹¬ë˜ëŠ” metadata í•„ë“œ ì•ˆì—ìˆëŠ” ì •ë³´ì…ë‹ˆë‹¤.
íŒŒì¼ì´ë¦„ì€ 'source'ì˜ .pdf í™•ì¥ì ì•ì—ìˆëŠ” ì´ë¦„ì´ê³ , í˜ì´ì§€ëŠ” 'page'ì˜ ìˆ«ìì— 1ì„ ë”í•˜ì„¸ìš”.

ì¶œë ¥ í˜•íƒœëŠ” 

[ì¶œì²˜]
íŒŒì¼ì´ë¦„: íŒŒì¼ì´ë¦„.pdf, í˜ì´ì§€: 15)

ì™€ ê°™ì´ í•´ì£¼ë©´ ë©ë‹ˆë‹¤.
ì—¬ëŸ¬ íŒŒì¼ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•˜ë‹¤ë©´ ìœ„ í˜•íƒœë¥¼ í™œìš©í•´ì„œ ì—¬ëŸ¬ê°œë¥¼ ëª¨ë‘ ëª…ì‹œí•´ì£¼ì„¸ìš”.

Question: {question} 
Context: {context} 
Answer:"""

st.set_page_config(page_title="safely ì±—ë´‡ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("safely ì±—ë´‡ í…ŒìŠ¤íŠ¸")


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]


def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


def plain_docs(docs):
    return docs


def format_docs_with_name_and_page(docs: List[Document]) -> str:
    formatted_texts = []

    for doc in docs:
        file_name = doc.metadata.get('source', 'unknown').split('/')[-1]
        page_number = doc.metadata.get('page', 'unknown') + 1  # í˜ì´ì§€ ë²ˆí˜¸ê°€ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ê²½ìš° +1
        page_content = doc.page_content

        formatted_text = f"íŒŒì¼ëª…: {file_name}, í˜ì´ì§€: {page_number}\në‚´ìš©:\n{page_content}\n"
        formatted_texts.append(formatted_text)

    return "\n".join(formatted_texts)


@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./safety_docs/pdf/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    # text_splitter = RecursiveCharacterTextSplitter(
    #     chunk_size=500,
    #     chunk_overlap=50,
    #     separators=["\n\n", "\n", "(?<=\. )", " ", ""],
    #     length_function=len,
    # )
    splitter = text_splitter.generate_text_splitter(2000, 200)
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = openai_embeddings_model.generate_embedding_model()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )

# if file:
#     # retriever = embed_file(file)
#     retriever = retriever

print_history()


if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        # eeve_llm = RemoteRunnable(LANGSERVE_ENDPOINT)
        eeve_llm = ollama_llm_model.eeve_llm
        # LM Studio ëª¨ë¸ ì„¤ì •
        # ollama = ChatOpenAI(
        #     base_url="http://localhost:1234/v1",
        #     api_key="lm-studio",
        #     model="teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf",
        #     streaming=True,
        #     callbacks=[StreamingStdOutCallbackHandler()],  # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€
        # )
        chat_container = st.empty()
        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        rerank_retriever = rerank_retriever.load_rerank_retriever(retriever)


        # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        rag_chain = (
                {
                    # "context": retriever | format_docs,
                    "context": rerank_retriever | format_docs_with_name_and_page,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | eeve_llm
                | StrOutputParser()
        )
        # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
        chunks = []
        for chunk in answer:
            chunks.append(chunk)
            chat_container.markdown("".join(chunks))
        add_history("ai", "".join(chunks))
